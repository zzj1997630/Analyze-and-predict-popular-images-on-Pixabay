{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import *\n",
    "import json\n",
    "import os\n",
    "#import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms, models\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torchvision import utils\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "import math\n",
    "Category=[\"backgrounds\", \"fashion\",\"nature\", \n",
    "\"science\", \"education\",\"feelings\", \"health\", \"people\", \"religion\", \"places\", \n",
    "\"animals\", \"industry\", \"computer\", \"food\",\"sports\", \"transportation\", \"travel\", \"buildings\", \"business\", \"music\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the URL for this pxiabay api code is provided in assignment\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "endpoint = \"https://pixabay.com/api\"\n",
    "\n",
    "headers= {\n",
    "\n",
    "}\n",
    "params={\n",
    "  \n",
    "    \"key\":\"26507728-f935f15cdfb5fb8ebc5a059cd\",\n",
    "    \"lang\":\"en\",\n",
    "    \"image_type\":\"photo\"\n",
    "    ,\"tags\":\"woman\"\n",
    "    ,\"page\":2\n",
    "    ,'per_page':200\n",
    "    ,'order':'popular'\n",
    "    ,\"category\":\"animal\"\n",
    "\n",
    "}\n",
    "views=0\n",
    "likes=0\n",
    "result = requests.get(endpoint, headers=headers, params=params)\n",
    "res = result.json()\n",
    "#print(res[\"total\"])\n",
    "for hit in res[\"hits\"]:\n",
    "    #print(\"URL：\"+hit[\"pageURL\"])\n",
    "  #  print(\"picture type：\"+hit[\"type\"])\n",
    "   # print(\"tag：\"+hit[\"tags\"])\n",
    "    #print(\"see the pictyre：\"+hit[\"previewURL\"])\n",
    "    #print(\"large image：\"+hit[\"largeImageURL\"])\n",
    "    views=views+hit[\"views\"]\n",
    "    likes=likes+hit['likes']\n",
    "id=hit['id']\n",
    "    #print(\"Views：\"+str(hit[\"views\"]))\n",
    "    #print(\"download：\"+str(hit[\"downloads\"]))\n",
    "    #print(\"likes：\"+str(hit[\"likes\"]))\n",
    "    #print(\"the number of comments:\"+str(hit['comments']))\n",
    "    #print(\"image size：\"+str(hit[\"imageSize\"]))\n",
    "    #print('published'+hit['category'])\n",
    "    #print(\"---------------------------\")\n",
    "#print(res['hits'][0])\n",
    "print('views : ',views)\n",
    "print('likes : ',likes)\n",
    "type(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bt\n",
    "import requests\n",
    "URL=\"https://jsonmock.hackerrank.com/api/countries/search?region={region}&name={keyword}&page={page_no}\"\n",
    "\n",
    "result=requests.get(URL)\n",
    "res=result.json()\n",
    "data=res[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': None, 'per_page': 10, 'total': 0, 'total_pages': 0, 'data': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10\n"
     ]
    }
   ],
   "source": [
    "c = [-10,-5,0,5,3,10,15,-20,25]\n",
    "print(max(c, key=lambda x: c.count(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 3 1 1]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = [0,1,2,2,3,4,4,4,5,6]\n",
    "print(np.bincount(array))\n",
    "print(np.argmax(np.bincount(array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  API_pixebay(name,number):\n",
    "    endpoint = \"https://pixabay.com/api\"\n",
    "    headers= {\n",
    "\n",
    "    }\n",
    "    params={\n",
    "    \"key\":\"26507728-f935f15cdfb5fb8ebc5a059cd\",\n",
    "    \"lang\":\"en\",\n",
    "    \"image_type\":\"photo\"\n",
    "    ,\"page\":number\n",
    "    ,'per_page':200\n",
    "    ,'category':name\n",
    "    ,'order':'popular'\n",
    "    }\n",
    "    result = requests.get(endpoint, headers=headers, params=params)\n",
    "    res = result.json()\n",
    "    return res\n",
    "def img(res,name,pat):\n",
    "        for i in range(len(res['hits'])):\n",
    "            a=res['hits'][i][\"largeImageURL\"]\n",
    "            response = requests.get(a)\n",
    "            filename = str(i)+\".jpg\"\n",
    "            with open(pat+filename, 'wb+') as f:\n",
    "                f.write(response.content)\n",
    "def json_document(res,pat):\n",
    "    pat=pat+'new_json.json'\n",
    "    b = json.dumps(res)\n",
    "    f2 = open(pat, 'w')\n",
    "    f2.write(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is downloading data part.It will take about 3 hours and  only ran once so I add \"#\" in front of them in case I run them again.\n",
    "#for i in range(len(Category)):\n",
    " #   for n in range(4):\n",
    "  #      if n !=0:\n",
    "   #         pat='D:/photo/dh/pic3/'+Category[i]+str(n)+'/'\n",
    "    #        if not os.path.exists(pat):\n",
    "     #           os.mkdir(path=pat)\n",
    "      #      res=API_pixebay(Category[i],n)\n",
    "        #    img(res,Category[i],pat)\n",
    "         #   json_document(res,pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublechose_views(views):\n",
    "    if views<=91732:\n",
    "        views=\"very unpopular\"\n",
    "    else:\n",
    "        views=\"very popular\"\n",
    "\n",
    "    return views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very popular\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#reading data part\n",
    "\n",
    "\n",
    "Category=[\"backgrounds\", \"fashion\",\"nature\", \n",
    "\"science\", \"education\",\"feelings\", \"health\", \"people\", \"religion\", \"places\", \n",
    "\"animals\", \"industry\", \"computer\", \"food\",\"sports\", \"transportation\", \"travel\", \"buildings\", \"business\", \"music\"]\n",
    "TAGS=[]\n",
    "for i in range(len(Category)):\n",
    "    for n in range(4):\n",
    "        if n !=0:\n",
    "            pat='D:/photo/dh/pic2/'+Category[i]+str(n)+'/'\n",
    "            JSONdocument = open(pat+'new_json.json')\n",
    "            popular_json = json.load(JSONdocument)\n",
    "            popular=popular_json['hits']\n",
    "            for hit in popular:\n",
    "                LIKE=doublechose_views(hit['views'])\n",
    "                TAGS.append(LIKE)\n",
    "location=[]\n",
    "for cat in range(len(Category)):\n",
    "    for num in range(4):\n",
    "        if num !=0:\n",
    "            pat='D:/photo/dh/pic2/'+Category[cat]+str(num)+'/'\n",
    "            for s in range(200):\n",
    "                singlelocation=pat+str(s)+'.jpg'\n",
    "                location.append(singlelocation)\n",
    "for i in range(len(Category)):\n",
    "    for n in range(4):\n",
    "        if n !=0:\n",
    "            pat='D:/photo/dh/pic3/'+Category[i]+str(n)+'/'\n",
    "            JSONdocument = open(pat+'new_json.json')\n",
    "            popular_json = json.load(JSONdocument)\n",
    "            popular=popular_json['hits']\n",
    "            for hit in popular:\n",
    "                LIKE=doublechose_views(hit['views'])\n",
    "                TAGS.append(LIKE)\n",
    "\n",
    "for cat in range(len(Category)):\n",
    "    for num in range(4):\n",
    "        if num !=0:\n",
    "            pat='D:/photo/dh/pic3/'+Category[cat]+str(num)+'/'\n",
    "            for s in range(200):\n",
    "                singlelocation=pat+str(s)+'.jpg'\n",
    "                location.append(singlelocation)\n",
    "\n",
    "\n",
    "path_s = pd.Series(location)\n",
    "label_s = pd.Series(TAGS)\n",
    "df = pd.DataFrame()\n",
    "df['path'] = path_s\n",
    "df['label'] = label_s\n",
    "print(df['label'][0])\n",
    "df['label']=df['label'].map({'very unpopular':0,\"very popular\":1})\n",
    "print(df['label'][0])\n",
    "df.to_csv('D:/photo/dh/pic3/' + '\\\\doubleviews.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91732.35783333333"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAGS=[]\n",
    "for i in range(len(Category)):\n",
    "    for n in range(4):\n",
    "        if n !=0:\n",
    "            pat='D:/photo/dh/pic2/'+Category[i]+str(n)+'/'\n",
    "            JSONdocument = open(pat+'new_json.json')\n",
    "            popular_json = json.load(JSONdocument)\n",
    "            popular=popular_json['hits']\n",
    "            for hit in popular:\n",
    "                TAGS.append(hit['views'])\n",
    "for i in range(len(Category)):\n",
    "    for n in range(4):\n",
    "        if n !=0:\n",
    "            pat='D:/photo/dh/pic3/'+Category[i]+str(n)+'/'\n",
    "            JSONdocument = open(pat+'new_json.json')\n",
    "            popular_json = json.load(JSONdocument)\n",
    "            popular=popular_json['hits']\n",
    "            for hit in popular:\n",
    "                TAGS.append(hit['views'])\n",
    "\n",
    "TAGS.sort(reverse=True)\n",
    "mean(TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "24000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([18664.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,  5336.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_paths=[]\n",
    "img_ids=[]\n",
    "\n",
    "csv_path='D:/photo/dh/pic3/doubleviews.csv'\n",
    "data = np.array(pd.read_csv(csv_path, header=0)).transpose(1, 0)\n",
    "img_paths.extend(data[1])\n",
    "img_ids.extend(data[2])\n",
    "idlist=[]\n",
    "pathlist=[]\n",
    "for i in range(len(img_ids)):\n",
    "    if math.isnan(img_ids[i]) == False:\n",
    "        idlist.append(img_ids[i])\n",
    "        pathlist.append(img_paths[i])\n",
    "idlist = [int(idlist) for idlist in idlist]\n",
    "print(len(idlist))\n",
    "print(len(pathlist))\n",
    "plt.hist(idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7776666666666666\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for i in idlist:\n",
    "    if i==0:\n",
    "        a.append(i)\n",
    "print(len(a)/24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22233333333333333\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for i in idlist:\n",
    "    if i ==1:\n",
    "        a.append(i)\n",
    "print(len(a)/24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'img':pathlist,'id':idlist}\n",
    "df=pd.DataFrame(data)\n",
    "train_dataset = df.sample(frac=0.7, random_state=20)\n",
    "train_dataset=train_dataset.reset_index()\n",
    "val_dataset = df.drop(train_dataset.index)\n",
    "val_dataset=val_dataset.reset_index()\n",
    "train_x=[]\n",
    "train_y=[]\n",
    "test_x=[]\n",
    "test_y=[]\n",
    "for i in range(len(train_dataset)):\n",
    "    train_x.append(train_dataset['img'][i])\n",
    "    train_y.append(train_dataset['id'][i])\n",
    "for n in range(len(val_dataset)):\n",
    "    test_x.append(val_dataset['img'][n])\n",
    "    test_y.append(val_dataset['id'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDatasetFromImages_train():\n",
    "    def __init__(self, label_list, imglist, resize_height=512, resize_width=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): csv 文件路径\n",
    "            img_path (string): 图像文件所在路径\n",
    "            transform: transform 操作\n",
    "        \"\"\"\n",
    "        # 需要调整后的照片尺寸，我这里每张图片的大小尺寸不一致#\n",
    "        self.resize_height = resize_height\n",
    "        self.resize_width = resize_width\n",
    "        self.file_path = imglist\n",
    "        self.to_tensor = transforms.ToTensor() #将数据转换成tensor形式\n",
    "        #self.data_info = pd.read_csv(csv_path)  #header=None是去掉表头部分\n",
    "        # 文件第一列包含图像文件的名称\n",
    "        #self.image_arr = np.asarray(self.data_info.iloc[:, 0])  #self.data_info.iloc[1:,0表示读取第一列，从第二行开始一直读取到最后一行\n",
    "        # 第四列是图像的 label\n",
    "        self.label_arr = label_list\n",
    "        \n",
    "        self.data_len = len(self.label_arr)\n",
    "    def __getitem__(self, index):\n",
    "        # 从 image_arr中得到索引对应的文件名\n",
    "        single_image_name = self.file_path[index]\n",
    " \n",
    "        # 读取图像文件\n",
    "        img_as_img = Image.open(single_image_name).convert('RGB')\n",
    "        \n",
    "        #如果需要将RGB三通道的图片转换成灰度图片可参考下面两行\n",
    "        #if img_as_img.mode != 'L':\n",
    "         #  img_as_img = img_as_img.convert('L')\n",
    "        \n",
    "        #设置好需要转换的变量，还可以包括一系列的nomarlize等等操作\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),    # 水平翻转\n",
    "            transforms.RandomVerticalFlip(p=0.3),    # 垂直翻转\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ,transforms.RandomApply(torch.nn.ModuleList([transforms.RandomCrop(size=512)]), p=0.5),# 随机裁剪，发生的概率为0.5 \n",
    "            transforms.Resize(size=(512, 512)),\n",
    "            #transforms.RandomErasing(p=0.4)    # 先resize成一个 300x300 的图片\n",
    "   \n",
    "\n",
    "        ])\n",
    "        img_as_img = transform(img_as_img)\n",
    " \n",
    "        # 得到图像的 label\n",
    "        label1 = self.label_arr[index]\n",
    "        label_numpy=np.array(label1)\n",
    "        label_tensor=torch.from_numpy(label_numpy).to(torch.int64)\n",
    "        #transform2=transforms.ToTensor()\n",
    "        return (img_as_img, label_tensor)  #返回每一个index对应的图片数据和对应的label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th orignal code of this part is also from internet but I changed a lot ,however,I also lost the URl of this part \n",
    "class CreateDatasetFromImages_test():\n",
    "    def __init__(self, label_list, imglist, resize_height=512, resize_width=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): csv 文件路径\n",
    "            img_path (string): 图像文件所在路径\n",
    "            transform: transform 操作\n",
    "        \"\"\"\n",
    "        # resize all pictures\n",
    "        self.resize_height = resize_height\n",
    "        self.resize_width = resize_width\n",
    "        self.file_path = imglist\n",
    "        self.to_tensor = transforms.ToTensor() #将数据转换成tensor形式\n",
    "      \n",
    "        self.label_arr = label_list\n",
    "        \n",
    "        self.data_len = len(self.label_arr)\n",
    "    def __getitem__(self, index):\n",
    "        # get the index \n",
    "        single_image_name = self.file_path[index]\n",
    " \n",
    "        # read pic\n",
    "        img_as_img = Image.open(single_image_name).convert('RGB')\n",
    "        \n",
    "        \n",
    "        #if img_as_img.mode != 'L':\n",
    "         #  img_as_img = img_as_img.convert('L')\n",
    "        \n",
    "        #nomarlize\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            \n",
    "\n",
    "        ])\n",
    "        img_as_img = transform(img_as_img)\n",
    " \n",
    "        # label\n",
    "        label1 = self.label_arr[index]\n",
    "        label_numpy=np.array(label1)\n",
    "        label_tensor=torch.from_numpy(label_numpy).to(torch.int64)\n",
    "        #transform2=transforms.ToTensor()\n",
    "        return (img_as_img, label_tensor)  #return the picture and their label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=CreateDatasetFromImages_train(train_y,train_x)\n",
    "valdata=CreateDatasetFromImages_test(test_y,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=len(train_x)\n",
    "val_size=len(test_x)\n",
    "train_loaderfor = torch.utils.data.DataLoader(num_workers=0,pin_memory=True,\n",
    "\n",
    "        dataset=traindata,\n",
    "        batch_size=8, \n",
    "        shuffle=False,\n",
    "        )\n",
    "val_loaderfor =torch.utils.data.DataLoader(num_workers=0,\n",
    "pin_memory=True,\n",
    "        dataset=valdata,\n",
    "        batch_size=8, \n",
    "        shuffle=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "#change self.implanes=512\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=5, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "                               #修改kernal_size from 7 to 3\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "       \n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# ResNet18/34的残差结构，用的是2个3x3的卷积\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # 残差结构中，主分支的卷积核个数是否发生变化，不变则为1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):  # downsample对应虚线残差结构\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:  # 虚线残差结构，需要下采样\n",
    "            identity = self.downsample(x)  # 捷径分支 short cut\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# ResNet50/101/152的残差结构，用的是1x1+3x3+1x1的卷积\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4  # 残差结构中第三层卷积核个数是第一/二层卷积核个数的4倍\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel * self.expansion,\n",
    "                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)  # 捷径分支 short cut\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    # block = BasicBlock or Bottleneck\n",
    "    # block_num为残差结构中conv2_x~conv5_x中残差块个数，是一个列表\n",
    "    def __init__(self, block, blocks_num, num_classes=1000, include_top=True,dropout_p=0.5):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])             # conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)  # conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)  # conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)  # conv5_x\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
    "        self.softmax =  nn.LogSoftmax(dim=1)\n",
    "       \n",
    "        #self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        #self.last_linear = nn.Linear(1000, 2)\n",
    "        #self.last_linear.weight.data.normal_(0, 0.01)\n",
    "        #self.last_linear.bias.data.fill_(0.0)\n",
    "\n",
    "    # channel为残差结构中第一层卷积核个数\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "\n",
    "        # ResNet50/101/152的残差结构，block.expansion=4\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel, channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet34(num_classes=4, include_top=True):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet101(num_classes=4, include_top=True):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import collections\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None,dropout_p=0.5):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=5, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
    "        #self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        #self.last_linear = nn.Linear(1000, 512)\n",
    "        #self.last_linear.weight.data.normal_(0, 0.01)\n",
    "        #self.last_linear.bias.data.fill_(0.0)\n",
    "        #self.last_linear1=nn.Linear(512, 256)\n",
    "        #self.last_linear2=nn.Linear(128, 64)\n",
    "        #self.last_linear3=nn.Linear(64,2),\n",
    "        #self.softmax =  nn.LogSoftmax(dim=1)\n",
    "  \n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.mean([2,3])\n",
    "        x = self.fc(x)\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = model_zoo.load_url(model_urls[arch], model_dir='.')\n",
    "        fsd = collections.OrderedDict()\n",
    "        res_iter = state_dict.items()\n",
    "        for i in range(len(res_iter)):\n",
    "            temp_key = list(res_iter)[i][0]\n",
    "            if 'fc' in temp_key:\n",
    "                continue\n",
    "            fsd[temp_key] = list(res_iter)[i][1]\n",
    "        model.load_state_dict(fsd, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for test 12000 pictures step by step. this research didn't use this method at last becasue it is too slow so the model part didn't show and it is similar to the previous one.\n",
    "EPOCHS = 40\n",
    "steps = 200\n",
    "\n",
    "for step in range(0,len(pathlist),steps):\n",
    "    imgs = []\n",
    "    train_pic=[]\n",
    "    val_pic=[]\n",
    "    for i in pathlist[step:step+steps]:\n",
    "        img = Image.open(i)\n",
    "        imgs.append(img)\n",
    "    data={'img':imgs,'id':idlist[step:step+steps]}\n",
    "    df=pd.DataFrame(data)\n",
    "    train_dataset = df.sample(frac=0.8, random_state=22)\n",
    "    train_dataset=train_dataset.reset_index()\n",
    "    val_dataset = df.drop(train_dataset.index)\n",
    "    val_dataset=val_dataset.reset_index()\n",
    "    for i in train_dataset['img']:\n",
    "        train_pic.append(img_transforms(i))\n",
    "    for nimg in val_dataset['img']:\n",
    "        val_pic.append(img_test_transforms(nimg))\n",
    "    traindataset = MyDataSet(train_pic,train_dataset['id'] )\n",
    "    valdataset=MyDataSet(val_pic,val_dataset['id'])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=traindataset,\n",
    "        batch_size=4, \n",
    "        shuffle=False,)\n",
    "    val_loader=torch.utils.data.DataLoader(\n",
    "        dataset=valdataset,\n",
    "        batch_size=4, \n",
    "        shuffle=False,)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "        val(model, DEVICE, val_loader)\n",
    "    #x.append(epoch) #此步为更新迭代步数\n",
    "    #t_loss.append(train_loss)\n",
    "    #v_loss.append(val_loss)\n",
    "torch.save(model, 'resnet34-9.28.'\n",
    "                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "train_num = train_size\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "val_num=val_size\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "loss_funcion = nn.NLLLoss()\n",
    "#loss_func.requres_grad = True\n",
    "net = resnet34()\n",
    "#change to resnet from 34 50\n",
    "\n",
    "# load pretrain weights\n",
    "#model_weight_path = \"C:/Users/lenovo/Downloads/resnet34-333f7ec4.pth\"\n",
    "#missing_keys, unexpected_keys = net.load_state_dict(torch.load(model_weight_path), strict=False)\n",
    "#for param in net.parameters():\n",
    " #    param.requires_grad = False\n",
    "# change fc layer structure\n",
    "inchannel = net.fc.in_features\n",
    "net.fc = nn.Sequential(\n",
    "    nn.Linear(inchannel, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256,4),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "#nn.Linear(inchannel, 4)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "#loss = Variable(loss_func, requires_grad = True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.000001,)\n",
    "def train_and_valid(net, loss_function, optimizer, epochs=25):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    " \n",
    "        net.train()\n",
    " \n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    " \n",
    "        for i, (inputs, labels) in enumerate(train_loaderfor):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    " \n",
    "            #因为这里梯度是累加的，所以每次记得清零\n",
    "            optimizer.zero_grad()\n",
    " \n",
    "            outputs = net(inputs)\n",
    " \n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.requires_grad_()\n",
    " \n",
    "            loss.backward()\n",
    " \n",
    "            optimizer.step()\n",
    " \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    " \n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    " \n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    " \n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    " \n",
    "            for j, (inputs, labels) in enumerate(val_loaderfor):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.cuda()\n",
    "                outputs = net(inputs)\n",
    " \n",
    "                loss = loss_function(outputs, labels)\n",
    " \n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    " \n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    " \n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    " \n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    " \n",
    "        avg_train_loss = train_loss/train_size\n",
    "        avg_train_acc = train_acc/train_size\n",
    " \n",
    "        avg_valid_loss = valid_loss/val_size\n",
    "        avg_valid_acc = valid_acc/val_size\n",
    " \n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    " \n",
    "        if best_acc < avg_valid_acc:\n",
    "            best_acc = avg_valid_acc\n",
    "            best_epoch = epoch + 1\n",
    " \n",
    "        epoch_end = time.time()\n",
    " \n",
    "        print(\"Epoch: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation: Loss: {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "            epoch+1, avg_valid_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start\n",
    "        ))\n",
    "        print(\"Best Accuracy for validation : {:.4f} at epoch {:03d}\".format(best_acc, best_epoch))\n",
    " \n",
    "        torch.save(net, str(epoch+1)+'.pt')\n",
    "    return net, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "Epoch: 001, Training: Loss: 0.5572, Accuracy: 73.6190%, \n",
      "\t\tValidation: Loss: 0.5572, Accuracy: 79.3333%, Time: 1437.6708s\n",
      "Best Accuracy for validation : 0.7933 at epoch 001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this is also from the internet but I forgot the website url\n",
    "num_epochs = 1\n",
    "trained_model, history = train_and_valid(net, loss_funcion, optimizer, num_epochs)\n",
    "#torch.save(history, 'D:/photo/'+'_history.pt')\n",
    "#torch.save(trained_model,'D:/photo/net3.pkl')\n",
    "history = np.array(history)\n",
    "plt.plot(history[:, 0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 1)\n",
    "#plt.savefig('D:/photo/'+'25epochviews_loss_curve.png')\n",
    "plt.show()\n",
    "plt.plot(history[:, 2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)          \n",
    "#plt.savefig('D:/photo/'+'25epochviews_accuracy_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(trained_model,'D:/photo/net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a popular picture\n"
     ]
    }
   ],
   "source": [
    "img_as_img = Image.open('D:/photo/dh/pic2/backgrounds1/64.jpg').convert('RGB')\n",
    "        \n",
    "        \n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "testimg = transform(img_as_img).float()\n",
    "test = testimg.unsqueeze(0)\n",
    "input = Variable(test)\n",
    "input = input.to(device)\n",
    "output = trained_model(input)\n",
    "index = output.data.cpu().numpy().argmax()\n",
    "if index==0:\n",
    "    print('this is not a popular picture')\n",
    "else:\n",
    "    print(\"this is a popular picture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('zijian')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e92a5b951864b817f503e4b4aff3d207d4d7f26758c4cb2ea36d2ac7a108b8d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
